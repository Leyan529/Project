{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Program\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Flatten, Activation\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
    "from keras.preprocessing.image import load_img, img_to_array\n",
    "from keras.utils import np_utils\n",
    "from keras.applications.resnet50 import ResNet50\n",
    "from keras.callbacks import *\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.image import load_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import dlib\n",
    "import cv2\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import PIL\n",
    "from PIL import Image\n",
    "import imutils\n",
    "from imutils.face_utils import *\n",
    "import os\n",
    "from os import listdir\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import sys\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from imutils.face_utils import FaceAligner\n",
    "from imutils.face_utils import rect_to_bb\n",
    "% matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#%%\n",
    "global detector ,landmark_predictor\n",
    "#宣告臉部偵測器，以及載入預訓練的臉部特徵點模型\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "landmark_predictor = dlib.shape_predictor('./face_model/shape_predictor_68_face_landmarks.dat')\n",
    "\n",
    "#%% 臉部識別函數宣告\n",
    "#讀取評分數據\n",
    "rating_dict={}\n",
    "with open('./All_labels.txt','rb') as label:\n",
    "    datalines = label.readlines()\n",
    "    for d in datalines:\n",
    "        d = str(d).replace('b','').replace('\\\\n','').replace(\"'\",\"\").split(' ')\n",
    "        rating_dict[d[0]] = float(d[1])  \n",
    "        \n",
    "def get_face(img):\n",
    "    #產生臉部識別\n",
    "    face_rects = detector(img, 1)\n",
    "    for i, d in enumerate(face_rects):\n",
    "        #讀取框左上右下座標\n",
    "        x1 = d.left()\n",
    "        y1 = d.top()\n",
    "        x2 = d.right()\n",
    "        y2 = d.bottom()\n",
    "        #根據此座標範圍讀取臉部特徵點\n",
    "        shape = landmark_predictor(img, d)\n",
    "        #將特徵點轉為numpy\n",
    "        shape = shape_to_np(shape)# (68,2)    \n",
    "        # 透過dlib挖取臉孔部分，將臉孔圖片縮放至256*256的大小，並存放於pickle檔中\n",
    "        # 人臉圖像部分呢。很簡單，只要根據畫框的位置切取即可crop_img = img[y1:y2, x1:x2, :]\n",
    "        crop_img = img[y1:y2, x1:x2, :]   \n",
    "        try:\n",
    "            crop_img = cv2.resize(crop_img, (128, 128))         \n",
    "            return crop_img   \n",
    "        except:\n",
    "            return np.array([0])  \n",
    "    return np.array([0])         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#%% 顏值特徵擷取\n",
    "def load_dataset():  \n",
    "    files = './Images'\n",
    "    image_data_list = []\n",
    "    label = []\n",
    "    start = time.time()\n",
    "    # 以迴圈處理\n",
    "    error = []\n",
    "    for idx,f in enumerate(os.listdir(files)):\n",
    "        # 產生檔案的絕對路徑\n",
    "        fullpath = os.path.join(files, f)\n",
    "        img = cv2.imread(fullpath)\n",
    "        face = get_face(img)\n",
    "        if (face.shape != (1,)) :\n",
    "            image_data_list.append(img_to_array(face))\n",
    "            label.append(rating_dict[f]) \n",
    "        else:\n",
    "            error.append(f)\n",
    "            print(fullpath)\n",
    "        if (idx%100==0)and (idx>0):        \n",
    "            print(\"{} detect success , use time:{:2f}s\".format(idx - len(error),time.time() - start))\n",
    "        del fullpath,img,face\n",
    "        \n",
    "    img_data = np.array(image_data_list)\n",
    "    img_data = img_data.astype('float32')\n",
    "    img_data /= 255        \n",
    "    return img_data, label "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5483"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#%% DataSet輸出為npz檔\n",
    "if not os.path.exists('./Image.npz'):\n",
    "    train_x, train_y = load_dataset()\n",
    "    train_x.shape\n",
    "else:\n",
    "    ds = np.load('./Image.npz')\n",
    "    train_x,train_y = ds['data'],ds['label']\n",
    "    len(rating_dict.keys())\n",
    "\n",
    "\n",
    "np.savez('./Image.npz', data=train_x, label=train_y)\n",
    "ds = np.load('./Image.npz')\n",
    "train_x,train_y = ds['data'],ds['label']\n",
    "train_y = np.array(train_y)\n",
    "len(train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([<matplotlib.axis.XTick at 0x1fd18254a58>,\n",
       "  <matplotlib.axis.XTick at 0x1fd1826c470>,\n",
       "  <matplotlib.axis.XTick at 0x1fd18254ac8>,\n",
       "  <matplotlib.axis.XTick at 0x1fd1a2e7e10>,\n",
       "  <matplotlib.axis.XTick at 0x1fd1a2f5828>,\n",
       "  <matplotlib.axis.XTick at 0x1fd18266470>,\n",
       "  <matplotlib.axis.XTick at 0x1fd182451d0>,\n",
       "  <matplotlib.axis.XTick at 0x1fd1a2fb0b8>,\n",
       "  <matplotlib.axis.XTick at 0x1fd1a2fb9b0>],\n",
       " <a list of 9 Text xticklabel objects>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYEAAAD9CAYAAABazssqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1fd141d5ba8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#%% 查看顏值分布情況\n",
    "scores = list(train_y)\n",
    "lv1 = [x for x in scores if x<1]\n",
    "lv2 = [x for x in scores if x>=1 and x<1.5]\n",
    "lv3 = [x for x in scores if x>=1.5 and x<2]\n",
    "lv4 = [x for x in scores if x>=2 and x<2.5]\n",
    "lv5 = [x for x in scores if x>=2.5 and x<3]\n",
    "lv6 = [x for x in scores if x>=3 and x<3.5]\n",
    "lv7 = [x for x in scores if x>=3.5 and x<4]\n",
    "lv8 = [x for x in scores if x>=4 and x<4.5]\n",
    "lv9 = [x for x in scores if x>=4.5]\n",
    "lv = [1,2,3,4,5,6,7,8,9]\n",
    "lv_label = ['lv1','lv2','lv3','lv4','lv5','lv6','lv7','lv8','lv9']\n",
    "len_lv = [len(x) for x in [lv1,lv2,lv3,lv4,lv5,lv6,lv7,lv8,lv9]]\n",
    "plt.bar(lv,len_lv)\n",
    "plt.xticks(lv, lv_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "seed = 42\n",
    "x_train_all, x_test, y_train_all, y_test = train_test_split(train_x, np.array(train_y), test_size=0.2, random_state=seed)\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train_all, y_train_all, test_size=0.2, random_state=seed)\n",
    "#%%\n",
    "def make_network():\n",
    "    resnet = ResNet50(include_top=False, pooling='avg', input_shape=(128, 128, 3))\n",
    "    model = Sequential()\n",
    "    model.add(resnet)\n",
    "    model.add(Dense(1))\n",
    "    model.layers[0].trainable = True\n",
    "#     model.compile(loss='mse', optimizer='adam')    \n",
    "    model.compile(loss='mse', optimizer='Adam',metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "resnet50 (Model)             (None, 2048)              23587712  \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 2049      \n",
      "=================================================================\n",
      "Total params: 23,589,761\n",
      "Trainable params: 23,536,641\n",
      "Non-trainable params: 53,120\n",
      "_________________________________________________________________\n",
      "Train on 3508 samples, validate on 878 samples\n",
      "Epoch 1/30\n",
      "  96/3508 [..............................] - ETA: 2:19:49 - loss: 8.2281 - acc: 0.0104    "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-e02cfd721a86>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     24\u001b[0m                           \u001b[0mvalidation_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m                           \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m                           callbacks = callback_list)\n\u001b[0m",
      "\u001b[1;32mD:\\Program\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1039\u001b[1;33m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1040\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32mD:\\Program\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Program\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2713\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2714\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2715\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2716\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2717\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Program\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2674\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2675\u001b[1;33m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2676\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Program\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1380\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[0;32m   1381\u001b[0m               \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1382\u001b[1;33m               run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1383\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1384\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = make_network()\n",
    "\n",
    "#%% 模型訓練\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss', \n",
    "    patience = 2, \n",
    "    verbose = 0, \n",
    "    mode='auto'\n",
    ")\n",
    "\n",
    "filepath=\"{epoch:02d}-{val_loss:.2f}.h5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "reduce_learning_rate = ReduceLROnPlateau(monitor='loss',\n",
    "                                         factor=0.1,\n",
    "                                         patience=2,\n",
    "                                         cooldown=2,\n",
    "                                         min_lr=0.00001,\n",
    "                                         verbose=1)\n",
    "\n",
    "callback_list = [checkpoint, reduce_learning_rate,early_stopping]\n",
    "\n",
    "train_history = model.fit(x_train, y_train, \n",
    "                          batch_size=8, epochs=30, verbose=1, \n",
    "                          validation_split=0.2,\n",
    "                          validation_data=(x_val, y_val),\n",
    "                          callbacks = callback_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% 模型評估\n",
    "def show_train_history(train_history, train, validation):\n",
    "    plt.plot(train_history.history[train])\n",
    "    plt.plot(train_history.history[validation])\n",
    "    plt.title('Train History')\n",
    "    plt.ylabel('train')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['train', 'validation'], loc='center right')\n",
    "    plt.show()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_train_history(train_history, 'acc','val_acc')\n",
    "show_train_history(train_history, 'loss','val_loss')\n",
    "model.evaluate(x_test,y_test)\n",
    "#%%\n",
    "plt.scatter(y_test,model.predict(x_test))\n",
    "plt.plot(y_test,y_test,'ro')\n",
    "#%% 儲存模型%權重\n",
    "# Save model\n",
    "model.save('./faceRank.h5')\n",
    "model.save_weights('./faceRank_weights.h5')\n",
    "del model\n",
    "from keras.models import load_model\n",
    "global model\n",
    "model = load_model('./faceRank.h5')\n",
    "model.load_weights('./faceRank_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#%% 目標顏值預測\n",
    "def predict_image(img_url):\n",
    "    try:\n",
    "        image = cv2.imread(img_url)\n",
    "        face = get_face(image)\n",
    "        face = face.astype('float32')\n",
    "        face /= 255  \n",
    "        print(face.shape)\n",
    "        image = img_to_array(face)\n",
    "        img = image[np.newaxis,:,:]\n",
    "        plt.axis('off')\n",
    "        plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "        print(\"Predict Score : {}\".format(model.predict(img)[0][0] * 20))  \n",
    "    except Exception as e :\n",
    "        print(e)\n",
    "        print('臉部辨識失敗')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "files = './Images'\n",
    "for idx,f in enumerate(os.listdir(files)[-5:-1]):\n",
    "    # 產生檔案的絕對路徑\n",
    "    fullpath = os.path.join(files, f)\n",
    "    predict_image(fullpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "files = './TestImages'\n",
    "for idx,f in enumerate(os.listdir(files)):\n",
    "    # 產生檔案的絕對路徑\n",
    "    fullpath = os.path.join(files, f)\n",
    "    img = load_img(fullpath)\n",
    "    plt.imshow(img)\n",
    "    predict_image(fullpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
